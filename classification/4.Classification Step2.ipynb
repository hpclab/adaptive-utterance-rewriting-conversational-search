{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_df = pd.read_pickle(\"../data/datasets/train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/datasets/test_df.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_features = pd.read_pickle(\"../data/gbdt_features/train_features_step2_all_feat_isolation.pkl\")\n",
    "test_features= pd.read_pickle(\"../data/gbdt_features/test_features_step2_all_feat_isolation.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1564, 3)\n",
      "(194, 3)\n",
      "(1564, 64)\n",
      "(194, 64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DROP SE lines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# DROP train lines (for examples labeled SE)\n",
    "drop_train_lines = train_df.index[train_df[2] == \"SE\"].tolist()\n",
    "print(drop_train_lines)\n",
    "\n",
    "s2_train_features = train_features.drop(drop_train_lines)\n",
    "s2_train_df = train_df.drop(drop_train_lines)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 19, 23, 24, 25, 26, 28, 29, 34, 37, 43, 44, 53, 54, 55, 57, 59, 62, 63, 67, 73, 76, 77, 78, 79, 80, 83, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 105, 106, 115, 126, 128, 130, 133, 135, 139, 140, 141, 142, 150, 151, 153, 154, 156, 158, 159, 160, 161, 168, 172, 173, 177, 179, 180, 181, 183, 188, 190, 193, 195, 196, 198, 202, 204, 206, 209, 210, 212, 214, 218, 222, 231, 234, 239, 246, 250, 253, 262, 269, 278, 285, 287, 289, 290, 296, 297, 299, 301, 302, 306, 308, 309, 312, 315, 316, 320, 322, 323, 327, 331, 334, 339, 342, 345, 347, 349, 350, 356, 357, 361, 362, 364, 368, 371, 373, 374, 383, 385, 388, 389, 392, 393, 396, 398, 402, 406, 408, 410, 412, 417, 419, 420, 422, 426, 427, 429, 436, 440, 446, 447, 450, 461, 465, 466, 467, 468, 471, 480, 483, 484, 486, 490, 497, 502, 508, 510, 512, 514, 517, 520, 522, 523, 532, 534, 540, 542, 544, 554, 559, 564, 569, 572, 574, 579, 584, 588, 589, 590, 591, 593, 594, 599, 600, 601, 603, 604, 609, 614, 618, 622, 626, 627, 631, 632, 636, 641, 644, 646, 649, 650, 654, 657, 659, 663, 664, 669, 674, 679, 680, 681, 682, 683, 684, 689, 694, 698, 703, 705, 706, 711, 716, 721, 725, 728, 730, 735, 740, 745, 750, 755, 756, 757, 758, 759, 760, 765, 769, 774, 779, 784, 789, 794, 797, 798, 802, 807, 812, 816, 817, 821, 826, 831, 835, 840, 845, 849, 854, 858, 859, 860, 861, 862, 863, 864, 865, 866, 868, 872, 875, 876, 877, 882, 887, 892, 893, 894, 897, 902, 907, 912, 917, 922, 927, 932, 937, 941, 946, 947, 948, 950, 951, 956, 961, 965, 970, 973, 978, 979, 982, 983, 986, 988, 989, 992, 997, 1002, 1007, 1012, 1017, 1018, 1019, 1020, 1021, 1022, 1024, 1026, 1027, 1031, 1036, 1041, 1046, 1051, 1055, 1060, 1062, 1065, 1070, 1073, 1077, 1082, 1087, 1092, 1096, 1101, 1105, 1110, 1115, 1120, 1122, 1125, 1130, 1135, 1140, 1145, 1150, 1155, 1157, 1160, 1165, 1170, 1175, 1180, 1184, 1186, 1187, 1192, 1197, 1202, 1207, 1208, 1209, 1211, 1212, 1217, 1222, 1227, 1232, 1237, 1242, 1246, 1251, 1255, 1258, 1262, 1267, 1272, 1277, 1282, 1285, 1287, 1290, 1291, 1295, 1299, 1300, 1305, 1309, 1313, 1314, 1319, 1324, 1329, 1333, 1334, 1339, 1344, 1348, 1353, 1358, 1363, 1364, 1365, 1367, 1368, 1373, 1376, 1378, 1382, 1387, 1392, 1397, 1401, 1406, 1411, 1416, 1418, 1423, 1428, 1432, 1437, 1442, 1447, 1452, 1457, 1458, 1459, 1461, 1462, 1467, 1472, 1477, 1482, 1487, 1492, 1497, 1502, 1506, 1509, 1511, 1516, 1521, 1525, 1530, 1534, 1539, 1544, 1549, 1554, 1559]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# drop the rest of SE for eval\n",
    "drop_test_lines = test_df.index[test_df[2] == \"SE\"].tolist()\n",
    "print(drop_test_lines)\n",
    "print(len(drop_test_lines))\n",
    "\n",
    "s2_test_features = test_features.drop(drop_test_lines)\n",
    "s2_test_df = test_df.drop(drop_test_lines)\n",
    "print(s2_test_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 2, 5, 9, 10, 11, 14, 15, 20, 29, 30, 33, 39, 44, 47, 51, 54, 56, 57, 59, 61, 64, 71, 77, 81, 82, 83, 84, 90, 96, 98, 104, 106, 110, 112, 113, 114, 119, 120, 123, 124, 127, 132, 134, 137, 138, 140, 143, 145, 149, 155, 156, 165, 166, 169, 170, 172, 174, 175, 177, 178, 181, 183, 185, 186, 188, 190, 193]\n",
      "68\n",
      "         0                                        1   2\n",
      "1     31_2                         Is it treatable?  FT\n",
      "3     31_4                   What are its symptoms?  PT\n",
      "4     31_5             Can it spread to the throat?  PT\n",
      "6     31_7            What is the first sign of it?  PT\n",
      "7     31_8     Is it the same as esophageal cancer?  PT\n",
      "..     ...                                      ...  ..\n",
      "184  78_10        What is the best for weight loss?  PT\n",
      "187   79_3    What is the role of positivism in it?  FT\n",
      "189   79_5        How is his work related to Comte?  PT\n",
      "191   79_7              What is its main criticism?  PT\n",
      "192   79_8  How does it compare to conflict theory?  PT\n",
      "\n",
      "[126 rows x 3 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(train_features.shape) # incl. SE\n",
    "print(s2_train_features.shape) # without SE\n",
    "\n",
    "print(test_features.shape) # incl. SE\n",
    "print(s2_test_features.shape) # without SE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1564, 64)\n",
      "(1096, 64)\n",
      "(194, 64)\n",
      "(126, 64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# clean the df\n",
    "cols = train_features.columns[2:]\n",
    "print(cols)\n",
    "\n",
    "# train\n",
    "X_train_df = s2_train_features[cols]\n",
    "X_train = s2_train_features[cols].to_numpy()\n",
    "\n",
    "train_labels = s2_train_df[2].map({'FT': 1, 'PT': 0})\n",
    "y_train_df = train_labels\n",
    "y_train = train_labels.to_numpy()\n",
    "print(\"TRAIN positive examples {} FT in total examples {}\".format(sum(y_train), len(y_train)))\n",
    "\n",
    "# test WITH ALL UTT\n",
    "X_test_df = s2_test_features[cols]\n",
    "X_test = s2_test_features[cols].to_numpy()\n",
    "test_labels = s2_test_df[2].map({'FT': 1, 'PT': 0})\n",
    "y_test_df = test_labels\n",
    "y_test = test_labels.to_numpy()\n",
    "print(\"TEST positive examples {} FT in total examples {} \".format(sum(y_test), len(y_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['utt_len', 'num_tokens', 'complete_sent', 'question_mark', 'ner',\n",
      "       'ner_b', 'ner_tm_0', 'ner_tm_1', 'ner_tm_b', 'noun', 'noun_b', 'adj',\n",
      "       'adj_b', 'adj_comp', 'adj_comp_b', 'adv', 'adv_b', 'adv_comp',\n",
      "       'adv_comp_b', 'pron', 'pron_b', 'pron_3rd', 'pron_3rd_b', 'cue_ph',\n",
      "       'cue_ph_b', 'cue_kw', 'cue_kw_b', 'cue_ex', 'cue_ex_b', 'cue_comp',\n",
      "       'cue_comp_b', 'question', 'question_b', 'question_ph', 'question_ph_b',\n",
      "       'what', 'where', 'when', 'who', 'why', 'which', 'how', 'how_much',\n",
      "       'how_many', 'how_long', 'what_is', 'what_is_2', 'what_is_3',\n",
      "       'tell_me_question', 'n_chunks', 'question_ph_2', 'question_ph_2_b',\n",
      "       'ques_mark_it', 'turn', 'cosine_first', 'cosine_prev', 'is_next_first',\n",
      "       'is_next_prev', 'nc_cosine_first', 'nc_cosine_prev', 'is_next_of_SE',\n",
      "       'dist_last_SE'],\n",
      "      dtype='object')\n",
      "TRAIN positive examples 857 FT in total examples 1096\n",
      "TEST positive examples 69 FT in total examples 126 \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train_df, y_train_df)\n",
    "lgb_eval = lgb.Dataset(X_test_df, y_test_df, reference=lgb_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# specify your configurations as a dict\n",
    "print('Starting training...')\n",
    "lgb_estimator = lgb.LGBMClassifier(\n",
    "                                   boosting_type = 'gbdt',\n",
    "                                   objective = 'binary' ,\n",
    "                                   learning_rate = 0.01,\n",
    "                                   random_state = 42,\n",
    "                                   num_leaves = 64,\n",
    "                                   n_estimators = 1500,\n",
    "                                   min_child_samples = 5\n",
    "                                   )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting training...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "lgb_estimator.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.01, max_depth=-1,\n",
       "               min_child_samples=5, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=1500, n_jobs=-1, num_leaves=64, objective='binary',\n",
       "               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print_model = False\n",
    "\n",
    "if print_model:\n",
    "    print('Saving model...')\n",
    "    # # save model to file\n",
    "    lgb_estimator.booster_.save_model('../data/gbdt_models/Step2_lightGBM.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grid search "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "            \n",
    "# use a full grid over all parameters\n",
    "param_grid = {\n",
    "              'num_leaves': [32,64, 128],\n",
    "              'n_estimators': [1250, 1500, 1750, 2000, 2500],\n",
    "              'min_child_samples': [2, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "run_grid_search = False\n",
    "if run_grid_search:\n",
    "    # run grid search\n",
    "    grid_search = GridSearchCV(lgb_estimator, param_grid=param_grid, cv=10)\n",
    "    start = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "          % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "    report(grid_search.cv_results_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print('Starting predicting...')\n",
    "y_pred = lgb_estimator.predict(X_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting predicting...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "y_pred = [int(round(x)) for x in y_pred]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The rmse of prediction is: 0.5194624816493197\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy : \", accuracy_score(y_test, y_pred, normalize = True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy :  0.7301587301587301\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[36 21]\n",
      " [13 56]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[36 21]\n",
      " [13 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.68        57\n",
      "           1       0.73      0.81      0.77        69\n",
      "\n",
      "    accuracy                           0.73       126\n",
      "   macro avg       0.73      0.72      0.72       126\n",
      "weighted avg       0.73      0.73      0.73       126\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "test_index = list(s2_test_df.index)\n",
    "\n",
    "res = [(a,b,c) for (a,b,c) in zip(y_test, y_pred, test_index) if  a != b]\n",
    "print(lgb_estimator.__class__)\n",
    "print(\"Missclassification \",len(res), \"/\", len(y_pred))\n",
    "print(\"False positives \", sum([b for _,b,_ in res]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "Missclassification  34 / 126\n",
      "False positives  21\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "print(\"Misclassification cases -> go back to utterances\")\n",
    "print(\"y_test, y_pred, test_index (up to 194)\")\n",
    "for item in res:\n",
    "    print(item, s2_test_df[0].loc[item[2]], s2_test_df[1].loc[item[2]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Misclassification cases -> go back to utterances\n",
      "y_test, y_pred, test_index (up to 194)\n",
      "(0, 1, 4) 31_5 Can it spread to the throat?\n",
      "(1, 0, 13) 32_5 What's the biggest ever caught?\n",
      "(1, 0, 22) 33_3 How was it received?\n",
      "(1, 0, 23) 33_4 Did it win any awards?\n",
      "(0, 1, 25) 33_6 Who was the author and when what it published?\n",
      "(1, 0, 36) 34_7 What about environmental factors?\n",
      "(1, 0, 37) 34_8 What empires survived?\n",
      "(1, 0, 38) 34_9 What came after it?\n",
      "(0, 1, 46) 37_8 What were the similarities and differences between the studies?\n",
      "(0, 1, 55) 40_5 How has it been integrated into music education?\n",
      "(0, 1, 60) 40_10 What are its roots and what influenced it?\n",
      "(1, 0, 65) 49_5 What are its other competitors?\n",
      "(1, 0, 69) 49_9 How has it impacted society?\n",
      "(1, 0, 70) 49_10 How about dating and relationships?\n",
      "(0, 1, 76) 50_6 Why did it create tension with the US?\n",
      "(0, 1, 97) 56_8 What is the relationship to speciation?\n",
      "(1, 0, 108) 59_3 What is the ACL?\n",
      "(0, 1, 109) 59_4 What is an injury for it?\n",
      "(0, 1, 121) 61_8 Who are the important members?\n",
      "(0, 1, 126) 67_4 How is oxygen transported?\n",
      "(0, 1, 129) 67_7 Can it go away?\n",
      "(0, 1, 139) 68_6 Whatâ€™s the difference with Bologna?\n",
      "(0, 1, 141) 68_8 What is done with the whey after production?\n",
      "(1, 0, 142) 68_9 What are typical pasta dishes?\n",
      "(0, 1, 147) 69_3 How was it discovered?\n",
      "(0, 1, 148) 69_4 What are good sources in food?\n",
      "(0, 1, 151) 69_7 Why does it require a prescription in the UK?\n",
      "(0, 1, 152) 69_8 How can I increase my levels naturally?\n",
      "(0, 1, 153) 69_9 Is it effective for treating insomnia?\n",
      "(1, 0, 158) 75_4 When and how were they domesticated?\n",
      "(1, 0, 162) 75_8 Why is it eaten on Thanksgiving?\n",
      "(0, 1, 167) 77_3 How about goulash?\n",
      "(0, 1, 168) 77_4 What are popular ones in France?\n",
      "(0, 1, 189) 79_5 How is his work related to Comte?\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "from operator import itemgetter\n",
    "feat_importance = [(a,b) for a, b in zip(lgb_estimator.feature_importances_, cols)]\n",
    "feat_importance.sort(key=itemgetter(0),reverse=True)\n",
    "feat_importance"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(13954, 'cosine_prev'),\n",
       " (12297, 'is_next_prev'),\n",
       " (11379, 'is_next_first'),\n",
       " (9255, 'cosine_first'),\n",
       " (8511, 'utt_len'),\n",
       " (5284, 'turn'),\n",
       " (3359, 'dist_last_SE'),\n",
       " (3075, 'nc_cosine_prev'),\n",
       " (2940, 'ner_tm_1'),\n",
       " (2870, 'num_tokens'),\n",
       " (2091, 'ner_tm_0'),\n",
       " (1973, 'nc_cosine_first'),\n",
       " (1688, 'noun'),\n",
       " (1669, 'n_chunks'),\n",
       " (1516, 'adj_b'),\n",
       " (1384, 'adv'),\n",
       " (1105, 'pron'),\n",
       " (1088, 'complete_sent'),\n",
       " (965, 'question_b'),\n",
       " (924, 'ner'),\n",
       " (887, 'question'),\n",
       " (628, 'pron_3rd'),\n",
       " (549, 'what_is'),\n",
       " (542, 'is_next_of_SE'),\n",
       " (373, 'how'),\n",
       " (364, 'tell_me_question'),\n",
       " (360, 'where'),\n",
       " (356, 'when'),\n",
       " (343, 'question_ph'),\n",
       " (323, 'which'),\n",
       " (305, 'who'),\n",
       " (281, 'cue_ex'),\n",
       " (272, 'what_is_3'),\n",
       " (204, 'question_ph_2'),\n",
       " (190, 'how_much'),\n",
       " (182, 'why'),\n",
       " (176, 'adj_comp'),\n",
       " (158, 'cue_ph'),\n",
       " (132, 'ques_mark_it'),\n",
       " (107, 'cue_comp'),\n",
       " (105, 'what_is_2'),\n",
       " (69, 'noun_b'),\n",
       " (67, 'question_mark'),\n",
       " (67, 'adv_comp'),\n",
       " (42, 'how_many'),\n",
       " (37, 'ner_tm_b'),\n",
       " (22, 'pron_b'),\n",
       " (15, 'pron_3rd_b'),\n",
       " (13, 'adv_b'),\n",
       " (4, 'ner_b'),\n",
       " (0, 'adj'),\n",
       " (0, 'adj_comp_b'),\n",
       " (0, 'adv_comp_b'),\n",
       " (0, 'cue_ph_b'),\n",
       " (0, 'cue_kw'),\n",
       " (0, 'cue_kw_b'),\n",
       " (0, 'cue_ex_b'),\n",
       " (0, 'cue_comp_b'),\n",
       " (0, 'question_ph_b'),\n",
       " (0, 'what'),\n",
       " (0, 'how_long'),\n",
       " (0, 'question_ph_2_b')]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}